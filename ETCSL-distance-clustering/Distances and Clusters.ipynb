{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance Measurements: Sumerian Literature\n",
    "\n",
    "This is a work-in-progress Notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Data\n",
    "First read the directory with the cleaned ETCSL texts. These files contain lemmatization in ORACC (ePSD2 style). The texts list lemmatizations per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path =r'../Scrape-etcsl/cleaned/' # use your path\n",
    "allFiles = glob.glob(path + \"/*.txt\")\n",
    "list_ = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0)\n",
    "    list_.append(df)\n",
    "etcsl_data = pd.concat(list_)\n",
    "etcsl_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Document Term Matrix\n",
    "In order to transform this DataFrame into a proper Document Term Matrix we need to discard the columns `version` and `l_no` and concatenate all the text that belongs to a single composition. Some lines have no content in the `text` column - these lines need to be dropped.\n",
    "\n",
    "First select the relevant columns and drop the rows that have no text content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "etcsl_data = etcsl_data[['id_text', 'text_name', 'text']]\n",
    "etcsl_data = etcsl_data.dropna()\n",
    "etcsl_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the rows by `id_text` and apply the `join` function to the `text` column. Transform the aggregated data into a new DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "etcsl_bytext = etcsl_data['text'].groupby(etcsl_data['id_text']).apply(' '.join)\n",
    "etcsl_bytext_df = pd.DataFrame(etcsl_bytext)\n",
    "etcsl_bytext_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Create a DataFrame of `id_text` and `text_name` equivalencies, with `id_text` set as index (row names). Then merge this DataFrame with the the `etctsl_bytext_df` using the indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "etcsl_id_names = etcsl_data[['id_text', 'text_name']].drop_duplicates().set_index('id_text')\n",
    "etcsl_data_df = pd.merge(etcsl_id_names, etcsl_bytext_df, right_index=True, left_index=True)\n",
    "etcsl_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfrom the DataFrame into a Document Term Matrix (DTM) by using `CountVecorizer`. This function uses a Regular Expression (`token_pattern`) to indicate how to find the beginning and end of each word (or token). In lemmatized Sumerian, a space indicates the boundary between two lemmas. The expression `r.[^ ]+` means: any combination of characters, except the space.\n",
    "\n",
    "The output of the CountVectorizer (`etcsl_dtm`) is not in a human-readable format. It is transformed into another DataFrame, with the ETCSL numbers as index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(analyzer='word', token_pattern=r'[^ ]+')\n",
    "etcsl_dtm = cv.fit_transform(etcsl_data_df['text'])\n",
    "etcsl_dtm_df = pd.DataFrame(etcsl_dtm.toarray(), columns = cv.get_feature_names(), index = etcsl_data_df.index.values)\n",
    "etcsl_dtm_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Text Length\n",
    "The length of each composition in ETCSL may be computed by adding up all the numbers in a row of the Document Term Matrix. The text length will be used at various places in further computations. Text length is added as a column to etcsl_data_df for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "etcsl_data_df['length'] =  etcsl_dtm_df.sum(axis=1)\n",
    "etcsl_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize\n",
    "Each row in the Document Term Matrix may be perceived as a vector with 4296 positions. The length of each vector depends on the number of lemmatized words. In order to make the vectors more comparable, we may set vector length at 1. This is done by dividing each value by the length of the vector, which equals the square root of the sum of the squared values: \n",
    "\n",
    "vector length = $\\sqrt{\\sum_{i=1}^nX^2_i}$.\n",
    "\n",
    "We now have two versions of the same Document Term Matrix: `etcsl_dtm_df` (with regular vocabulary counts) and `etcsl_dtm_df_norm` with normalized counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "etcsl_dtm_df_norm = etcsl_dtm_df.apply(lambda x: (x / np.sqrt(sum(np.square(x)))), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminate very short compositions\n",
    "Some of the compositions in [ETCSL](etcsl.orinst.ox.ac.uk) are very short because they are only known from catalog texts or they exist only in a single (fragmentary) exemplar. Normalizing will dramatically increase the weight of the few words that appear in those texts. In addition to normalizing, therefore, we will also eliminate texts that have fewer than 50 lemmatized words. The regular DTM (`etcsl_dtm_df`) still contains all 394 compositions.\n",
    "\n",
    "The following 39 compositions have been eliminated from the normalized DTM (listed by length of the composition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "etcsl_dtm_df_norm = etcsl_dtm_df_norm[etcsl_data_df['length'] > 49]\n",
    "etcsl_data_df[['text_name', 'length']][etcsl_data_df['length'] < 50].sort_values(by='length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Distances and Clustering\n",
    "From here on use the explanations in the blog by [JÃ¶rn Hees](https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/) for hierarchical clustering and creating dendrograms for step 1 of the clustering analysis (step 2 is K-means clustering). Check also the text clustering Notebook by [Brandon Rose](http://brandonrose.org/clustering), which combines K-means, hierarchical clustering and topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, ward\n",
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "#import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some setting for this notebook to actually show the graphs inline, you probably won't need this. Disable the %matplotlib line in order to get \n",
    "# the interactive matplotlib GUI.\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision=5, suppress=True)  # suppress scientific float notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The function dendro()\n",
    "\n",
    "The function `dendro()` draws a dendrogram of a hierarchical clustering analysis, using the Ward method for the linkage and Euclidean distances. The `dendro()` function will be used to try different approaches to hierarchical clustering: clustering the entire corpus in raw counts; clustering with normalized vectors; and clustering with `tf-idf` weighted count. In addition, the function is used to cluster a sub-corpus: the heroic narratives around Gilgamesh, Enmerkar, and Lugalbanda.\n",
    "\n",
    "The `dendro()` function draws a horizontal dendrogram. It takes four parameters: the DTM that is to be analyzed; the labels to be used for each data point (a list, to be derived from the DTM), the label to be printed along the Y-axis (describing the dataset), and the height of the figure (an integer). The analysis of the entire corpus will produce a very large image. Right click the nimage and open it in a new browser to inspect it in more detail.\n",
    "\n",
    "The function returns the linkage matrix Z, which may be used in computing the Cophenetic Correlation Coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dendro(dtm, labels, ylabel, heigth = 10):\n",
    "# generate the linkage matrix\n",
    "    Z = linkage(dtm, 'ward')\n",
    "# calculate full dendrogram\n",
    "    plt.figure(figsize=(25, heigth))\n",
    "    plt.title('Hierarchical Clustering Dendrogram')\n",
    "    plt.xlabel('distance')\n",
    "    plt.ylabel(ylabel)\n",
    "    dendrogram(\n",
    "        Z,\n",
    "        labels = labels,\n",
    "        orientation = 'right',\n",
    "#        leaf_rotation=90.,  # rotates the x axis labels\n",
    "        leaf_font_size=10.,  # font size for the x axis labels\n",
    "    )\n",
    "    plt.show()\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Z = dendro(etcsl_dtm_df, etcsl_data_df['text_name'], 'full etcsl corpus\\nno normalization', 50)\n",
    "c, coph_dists = cophenet(Z, pdist(etcsl_dtm_df))\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = etcsl_data_df['text_name'][etcsl_data_df['length'] > 49]\n",
    "Z = dendro(etcsl_dtm_df_norm, labels, 'normalized etcsl corpus (vector length = 1)\\ntexs < 50 lemmatized words omitted', 50)\n",
    "c, coph_dists = cophenet(Z, pdist(etcsl_dtm_df_norm))\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select\n",
    "Select only the Gilgamesh, Enmerkar and Lugalbanda stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heroic = ['c.1.8.1.1', 'c.1.8.1.2', 'c.1.8.1.3', 'c.1.8.1.4', 'c.1.8.1.5', 'c.1.8.1.5.1', 'c.1.8.2.1', \n",
    "          'c.1.8.2.2', 'c.1.8.2.3', 'c.1.8.2.4'\n",
    "         ]\n",
    "labels = list(etcsl_data_df.ix[heroic]['text_name'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute a distance matrix [or perhaps skip this step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = etcsl_dtm_df.ix[heroic]\n",
    "dist = pdist(X)\n",
    "heroic_dist_df = pd.DataFrame(squareform(dist)).round(2)\n",
    "heroic_dist_df.index = labels\n",
    "heroic_dist_df.columns = labels\n",
    "heroic_dist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Z = dendro(etcsl_dtm_df.ix[heroic], labels, 'heroic narratives\\nno normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = etcsl_dtm_df_norm.ix[heroic]\n",
    "dist = pdist(X)\n",
    "heroic_dist_df = pd.DataFrame(squareform(dist)).round(2)\n",
    "heroic_dist_df.index = labels\n",
    "heroic_dist_df.columns = labels\n",
    "heroic_dist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Z = dendro(etcsl_dtm_df_norm.ix[heroic], labels, 'heroic narratives normalized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Is this due to personal names? Apparently not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "heroic_dtm = etcsl_dtm_df_norm.ix[heroic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qpn = {'cn', 'dn', 'en', 'fn', 'gn', 'mn', 'on', 'pn', 'rn', 'sn', 'tn', 'wn'}\n",
    "words = heroic_dtm.columns\n",
    "words_no_names = [word for word in words if not word[-2:] in qpn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heroic_no_names_dtm = heroic_dtm[words_no_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Z = dendro(heroic_no_names_dtm, labels, 'heroic narratives normalized \\n no proper names')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "the entire dendrogram without proper names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = etcsl_dtm_df_norm.columns\n",
    "words_no_names = [word for word in words if not word[-2:] in qpn]\n",
    "etcsl_no_names_dtm = etcsl_dtm_df_norm[words_no_names]\n",
    "labels = etcsl_data_df['text_name'][etcsl_data_df['length'] > 49]\n",
    "Z = dendro(etcsl_no_names_dtm, labels, 'etcsl normalized \\n no proper names', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "tf-idf is an alternative vectorizer that weighs words and normalizes for text length. Texts that are too short (< 50 lemmatized words) are omitted before applying `tf-idf`. Among the settings for `tf-idf` are `max_df` and `min_df` or maximum and minimum document frequency. Document frequency refers to the percentage of the documents that contain a certain term at least once. Common values are 80% and 20% - that is, terms that appear in more than 80% or less than 20% of the documents in the corpus are excluded from the analysis. This setting excludes far too many words for the present purpose (it keeps only 178 out of more than 4,200 unique words). Currently `max_df` and `min_df` are set to 98% and 2%, respectively (keeping 1185 unique terms) - but more research is needed to find an appropriate setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "long = etcsl_data_df['length'] > 49\n",
    "etcsl_noshort_df = etcsl_data_df[long]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.98, max_features=200000,\n",
    "                                 min_df=0.02,\n",
    "                                 use_idf=True, token_pattern=r'[^ ]+', ngram_range=(1,1))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(etcsl_noshort_df['text'])\n",
    "etcsl_tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns = tfidf_vectorizer.get_feature_names(), \n",
    "                              index = etcsl_noshort_df.index.values)\n",
    "etcsl_tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = etcsl_noshort_df['text_name']\n",
    "Z = dendro(etcsl_tfidf_df, labels, 'etcsl tfidf', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf-idf without proper nouns. Note that in this dendrogram *Enmerkar and Ensuhgirana* and *Enmerkar and the Lord of Aratta* end up in entirely different branches of the tree. The two *Lugalbanda* stories are still together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = etcsl_noshort_df['text_name']\n",
    "words = etcsl_tfidf_df.columns\n",
    "words_no_names = [word for word in words if not word[-2:] in qpn]\n",
    "Z = dendro(etcsl_tfidf_df[words_no_names], labels, 'etcsl tf-idf\\nno proper nouns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = list(etcsl_data_df.ix[heroic]['text_name'])\n",
    "Z = dendro(etcsl_tfidf_df.ix[heroic], labels, 'tf-idf heroic narratives')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "One can also look at n-grams using tf-idf (keep all other parameters the same)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.98, min_df=0.02,\n",
    "                                 use_idf=True, token_pattern=r'[^ ]+', ngram_range=(1,4))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(etcsl_noshort_df['text'])\n",
    "etcsl_tfidf_ngrams_df = pd.DataFrame(tfidf_matrix.toarray(), columns = tfidf_vectorizer.get_feature_names(), \n",
    "                              index = etcsl_noshort_df.index.values)\n",
    "etcsl_tfidf_ngrams_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives a very nice clustering - but one may ask whether some of that clustering is based primarily on PNs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = etcsl_noshort_df['text_name']\n",
    "Z = dendro(etcsl_tfidf_ngrams_df, labels, 'etcsl tfidf\\nn-gram range 1-4', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kicking the names out still gives an interesting picture. Note that Enmerkar and Ensuhgirana and Enmerkar and the Lord of Aratta do not cluster directly anymore - but are still pretty close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = etcsl_noshort_df['text_name']\n",
    "words = etcsl_tfidf_ngrams_df.columns\n",
    "words_no_names = [word for word in words if not word[-2:] in qpn]\n",
    "Z = dendro(etcsl_tfidf_ngrams_df[words_no_names], labels, 'etcsl tf-idf\\nn-grams 1-4\\nno proper nouns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = list(etcsl_data_df.ix[heroic]['text_name'])\n",
    "Z = dendro(etcsl_tfidf_ngrams_df.ix[heroic], labels, 'tf-idf n-grams 1-4\\nheroic narratives')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
