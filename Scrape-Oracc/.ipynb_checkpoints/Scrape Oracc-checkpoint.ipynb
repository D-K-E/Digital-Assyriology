{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "The scraper output needs a field 'version' to make it compatible with ETCSL output. This field may be used for pseudo-composites such as [dcclt/Q000043](http://oracc.org/dcclt/Q000043) to indicate the designation of the tablet from which the text is taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ORACC Scraper\n",
    "\n",
    "Generalized scraper for [ORACC](html://oracc.org) files. The scraper needs an input file, that lists the P, Q, or X numbers to be scraped. It will create an output file for each of these P, Q, or X numbers with line numbers and lemmatized words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment\n",
    "Import the packages:\n",
    "- re Regular Expressions (for string manipulation)\n",
    "- BeautifulSoup (for parsing HTML pages)\n",
    "- tqdm for progress bar\n",
    "This scraper is written for Python3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running under Python version: (3, 5, 1)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "#! pip install tqdm\n",
    "from __future__ import print_function\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "from time import sleep\n",
    "from tqdm import *\n",
    "\n",
    "#this program should use python unit test\n",
    "PY3 = sys.version_info.major == 3\n",
    "\n",
    "if not PY3:\n",
    "    input = raw_input\n",
    "\n",
    "print(\"Running under Python version:\", sys.version_info[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize output variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textiderror = 'Not available:\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input File\n",
    "\n",
    "The input file should be located in a directory called /Input, which must be located in the directory in which this Python Notebook is found. The file should have a .txt extension and must be created with a flat text editor such as TextEdit, Notepad, or Emacs. The file contains a simple list of P, Q, or X numbers, preceded by the ORACC abbreviation where the file is edited. For instance:\n",
    "\n",
    "    rinap/rinap1/Q003421\n",
    "    dcclt/Q000039\n",
    "    cams/gkab/P348623\n",
    "\n",
    "Before running this scraper, use the same input file to download the text material (in html format) with the ORACC Downloader to the /Data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of Input List: test.txt\n"
     ]
    }
   ],
   "source": [
    "inputfile = input(\"Name of Input List: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format Output\n",
    "\n",
    "The function outputformat() defines what the output of the lemmatized forms will look like. This function may be adapted in various ways to produce different types of output. The function takes a dictionary as input with the following keys: \n",
    "\n",
    "- 1     lang: Language\n",
    "- 2     translit: Transliteration\n",
    "- 3     citform: Citation Form\n",
    "- 4     guideword: Guide Word\n",
    "- 5     sense: Sense\n",
    "- 6     pos: Part of Speech\n",
    "- 7     epos: Effective Part of Speech\n",
    "- 8     norm: Normalization\n",
    "- 9     base: Base (Sumerian only)\n",
    "- 10    morph: Morphology (Sumerian only)\n",
    "\n",
    "In the standard format the output will look like: sux:lugal[king]N. One may adapt the output, for instance, by omitting the element lang (lugal[king]N) or to select for certain parts of speech, or for certain language codes. For instance:\n",
    "```python\n",
    "    if output['pos'] == 'N':\n",
    "        output_formatted = output['citform'] + \"\\t\" + output['guideword']\n",
    "```\n",
    "This will create output in the form lugal   king (lugal and king seperated by TAB), selecting only Nouns (excluding Proper Nouns).\n",
    "\n",
    "```python\n",
    "    if output['lang'] == 'sux-x-emesal':\n",
    "        output_formatted = output['citform'] + \"[\" + output['guideword'] + \"]\" + output['pos']\n",
    "```\n",
    "This will create output in the form umun[lord]N, selecting only Emesal words.\n",
    "In order to select both Sumerian (sux) and Emesal (sux-x-emesal) forms one could use:\n",
    "```python\n",
    "    if output['lang'][0:3] == 'sux':\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def outputformat(output):\n",
    "    #output_formatted = ''\n",
    "    if True:\n",
    "        output_formatted = output['lang'] + \":\" + output['citform'] + \"[\" + output['guideword'] + \"]\" + output['pos']\n",
    "    else:\n",
    "        return None\n",
    "    return output_formatted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse an ORACC lemma\n",
    "\n",
    "The function `parselemma()` parses a so-called ORACC 'signature.' It is called by the `getline()` function where these signatures are extracted from the html files. A signature, as extracted by the `getline()` function, looks as follows:\n",
    "\n",
    "> javascript:pop1sig('dcclt','','@dcclt%sux:am-si=amsi[elephant//elephant]N´N$amsi/am-si#~').\n",
    "\n",
    "Akkadian signatures look slightly different, lacking the last two elements (after the /). The `parselemma()` function removes all the superfluous elements and breaks the string up into its parts. It returns a dictionary that lists all of these parts. The function `getline()` forwards this dictionary to the function `formatoutput()`, which uses its keys and values to build a usable data structure and/or to filter the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parselemma(signature):\n",
    "    output = {}\n",
    "    signature = signature.replace(' ', '-')\n",
    "    signature = signature.replace(',', ';') #remove spaces and commas from GuideWord and Sense\n",
    "    oracc_words = re.sub(\"'\\)$\", \"\", signature) #remove ') from the end of the signature\n",
    "    oracc_words = re.sub('^.*%', '', oracc_words) #remove everything before % in the signature\n",
    "\n",
    "    \n",
    "    sep_char = [\":\", \"=\", \"$\", \"#\", \"[\", \"]\", \"//\"] # these are the character sequences that separate\n",
    "                                                    # the various elements of the signature\n",
    "    for eachchar in sep_char:\n",
    "        oracc_words = oracc_words.replace(eachchar, \" \", 1) # ':' may appear in guideword/sense, so replace only once\n",
    "    oracc_words_l = oracc_words.split() #split signature into a list\n",
    "    output['lang'] = oracc_words_l[0]\n",
    "    output['translit'] = oracc_words_l[1]\n",
    "    output['citform'] = oracc_words_l[2]\n",
    "    output['guideword'] = oracc_words_l[3]\n",
    "    output['sense'] = oracc_words_l[4]\n",
    "\n",
    "    oracc_words_l[5] = oracc_words_l[5].replace(\"´\", \" \") # this separates pos from epos\n",
    "    oracc_words_l[5] = oracc_words_l[5].split() #split into sub-list\n",
    "    output['pos'] = oracc_words_l[5][0]\n",
    "    output['epos'] = oracc_words_l[5][1]\n",
    "    \n",
    "    if output['lang'][0:2] == 'sux': # Sumerian or Emesal signature\n",
    "        oracc_words_l[6] = oracc_words_l[6].replace(\"/\", \" \") # this separates norm from base in sux\n",
    "        oracc_words_l[6] = oracc_words_l[6].split() #split into sub-list\n",
    "        output['norm'] = oracc_words_l[6][0]\n",
    "        output['base'] = oracc_words_l[6][1]\n",
    "        output['morph'] = oracc_words_l[7]\n",
    "    \n",
    "    else:\n",
    "        output['norm'] = oracc_words_l[6]\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compound Orthographic Forms\n",
    "\n",
    "Compound Orthographic Forms are combinations of two or more words that are written with a single graphemic complex. Examples are *im-ma-ti* for *ina mati* (when?) or {lu₂}EN.NAM for *bēl pīhati* (governor). In ORACC HTML, the words in a COF are combined in a single signature, separated by '&&':\n",
    "\n",
    "> javascript:pop1sig('saao/saa10','','@saao/saa10%akk-x-neoass:im-ma-ti=ina[in//in]PRP´PRP\\$ina&&@saao/saa10%akk-x-neoass:=mati[when?//when?]QP´QP\\$mati')\n",
    "\n",
    "The function `cof()` is called, when necessary, by `getline()`. It splits the signature at the '&&' separator and returns a list of signatures. The transliteration (in this case *im-ma-ti*), which is included only in the first signature, is isolated and assigned to the variable `translit`. This transliteration is inserted in the remaining signatures at the proper place.\n",
    "\n",
    "Occasionally, in some COF signatures, the second and further words do not have their own normalization (introduced by $) - this is, presumably, an irregularity in ORACC. If this is the case, normalization is supplied by assuming that it is equal to the citation form in the function `supplynorm()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cof(signature):\n",
    "    signature = signature.replace(\"javascript:pop1sig('\", \"\")\n",
    "    signature = signature.replace(\"')\", \"\")\n",
    "    translit = re.search(':(.*?)=', signature).group(1) #TODO replace the expression with positive look back\n",
    "                                                        # and positive look ahead expression\n",
    "    cofsignatures = signature.split('&&@')\n",
    "    cofsignatures = [cofsignature.replace(':=', ':' + translit + '=') for cofsignature in cofsignatures]\n",
    "\n",
    "    def supplynorm(cofsignature):\n",
    "        if not '$' in cofsignature:\n",
    "            citform = re.search('=(.*?)\\[', cofsignature).group(1)\n",
    "            cofsignature = cofsignature + '$' + citform\n",
    "        return cofsignature\n",
    "    \n",
    "    cofsignatures = [supplynorm(cofsignature) for cofsignature in cofsignatures]\n",
    "        \n",
    "    return cofsignatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process a Line\n",
    "\n",
    "The function `getline()` receives a line with metadata from the function `scrape()`. It returns a single variable (`line`) that contains the lines metadata and the formatted lemmata in a single string. \n",
    "\n",
    "The function needs two arguments. The first, `line_label`, includes all the metadata of the line: `id_text`, `text_name`, and `l_no` in a single string (separated by commas). The second argument, `line`, is a `BeautifulSoup` object that holds the HTML representation of a single line.\n",
    "\n",
    "Each line contains a series of words, represented as `signatures` in ORACC HTML. The function collects the signatures in the list `lemmas` and iterates over these. If a signature represents a Compound Orthographic Form (a combination of multiple lemmas, separated by '&&') it is sent to the function `cof()` in order to split the signature in its component forms.\n",
    "\n",
    "Each signature is sent to `parselemma()`, where it is analyzed. The function `parselemma()` returns a dictionary (`output`) that contains all the elements of the signature (transliteration, citation form, guide word, etc.). This dictionary is sent to the function `outputformat()` which returns a string, combining elements of the signature in the desired format (the default is language:citform[GuideWord]POS, as in sux:lugal[king]N). This string is added to the list `wordsinline`. Finally, once all lemmas (signatures) have been processed, the function builds a single string out of the `line_label` (metadata) and the formatted lemmas in `wordsinline`. This string is returned in the variable `line`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getline(line_label, line):\n",
    "    wordsinline = [] #initialize list for the words in this line\n",
    "    lemmas = line.findAll('a', {'class':'cbd'}, href=True)\n",
    "    for eachlemma in lemmas:\n",
    "        signature = eachlemma['href']\n",
    "        if '&&@' in signature:  #Compound Orthographic Form, which results in multiple lemmas\n",
    "            cofsignatures = cof(signature)\n",
    "            for cofsignature in cofsignatures:\n",
    "                output = parselemma(cofsignature)\n",
    "                output_formatted = outputformat(output)\n",
    "                if not output_formatted == None:\n",
    "                    wordsinline.append(output_formatted)\n",
    "        else:\n",
    "            output = parselemma(signature)\n",
    "            output_formatted = outputformat(output)\n",
    "            if not output_formatted == None:\n",
    "                wordsinline.append(output_formatted)\n",
    "    line = line_label + ' '.join(wordsinline)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape a Single Text\n",
    "\n",
    "The function `scrape()` takes a single text file and uses the `BeautifulSoup` package to analyze the HTML and return the data in a csv (Comma Separated Values) format. The function is called by the main process.\n",
    "\n",
    "The function `scrape()` first identifies the name (or designation) of the text - if it cannot find the name, the text is not further processed and an error message is returned.\n",
    "\n",
    "The function then identifies a line and sends this line to the function `getline()` for further processing. The line number is a string that belongs to the attribute `class = 'xlabel'`. Text name, text id and line number are combined into a single string as `line_label` - which is sent to `getline()` as its first argument (the second is the line itself).\n",
    "\n",
    "If there is no `class = 'xlabel'` attribute, this means that the line belongs with the previous line as a single unit. This happens in interlinear bilinguals and, occasionally, in the representation of explanatory glosses (see, e.g. SAA 10, 044). In such cases the variable `line` from the previous iteration (which is a single string, concatenating `line_label` and the formatted lemmas) is sent, in its entirety, as the first argument to `getline()` so that the new lemma or lemmas will be added to the end of it.\n",
    "\n",
    "Finally all lines are assembled in the variable `csvformat`, which is returned to the main process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scrape(text_id):\n",
    "    print('Parsing ' + text_id)\n",
    "    line = ''\n",
    "    csvformat ='id_text,text_name,l_no,text\\n' #initialize output variable\n",
    "    with open('HTML/' + text_id.replace('/', '_') + '.html') as f:\n",
    "        raw_html = f.read()\n",
    "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "    name = soup.find('h1', {'class':'p3h2'}).string\n",
    "    #if there is no text name, there are errors in atf and page was not built correctly\n",
    "    if name == None:\n",
    "        print(eachtextid + \" is not built correctly.\")\n",
    "    else:\n",
    "        #if name includes comma, replace comma with nothing\n",
    "        name = name.replace(',','')\n",
    "        print(eachtextid + \":\" + name)\n",
    "        lines = soup.findAll('tr', {'class':'l'})\n",
    "        for index, eachline in enumerate(lines):\n",
    "            if eachline.find('a', {'class':'cbd'}, href=True) == None: # if the line has no words\n",
    "                continue                                               # go to next line\n",
    "            if eachline.find('span', {'class': 'xlabel'}) != None:\n",
    "                l_no = eachline.find('span', {'class': 'xlabel'}).string.replace(',', ';')\n",
    "                line_label = text_id + ',' + name + ',' + l_no + ','\n",
    "                line = getline(line_label, eachline)\n",
    "                try:\n",
    "                    nextline = lines[index + 1]\n",
    "                    if nextline.find('span', {'class': 'xlabel'}) == None: #if next line has no line number\n",
    "                        if nextline.find('a', {'class':'cbd'}, href=True) == None: #ignore if there are no lemmatized words\n",
    "                            continue                                               # in next line\n",
    "                        else:\n",
    "                            line_label = line + ' '                         # otherwise join output with previous line\n",
    "                            line = getline(line_label, nextline)\n",
    "                except:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "            csvformat = csvformat + line + '\\n'\n",
    "    return csvformat\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Process\n",
    "\n",
    "The main process opens the list of text IDs (in the directory `Input`) to be processed and iterates over that list. The process calls the function `scrape()` which, in turn, calls the other functions defined above.\n",
    "\n",
    "The function creates a separate file for each of the scraped documents in the directory Output. A list of texts that could not be scraped is saved in the directory Error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:00<00:00,  9.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing rinap/rinap1/Q003421\n",
      "rinap/rinap1/Q003421:Tiglath-pileser III 08\n",
      "Saving Output/rinap_rinap1_Q003421.txt\n",
      "\n",
      "Parsing dcclt/Q000039\n",
      "dcclt/Q000039:OB Nippur Ura 01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 2/6 [00:02<00:02,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Output/dcclt_Q000039.txt\n",
      "\n",
      "Parsing cams/gkab/P348623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 3/6 [00:02<00:01,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cams/gkab/P348623:SpTU 2 018 [Namburbu]\n",
      "Saving Output/cams_gkab_P348623.txt\n",
      "\n",
      "Parsing saao/saa10/P334751\n",
      "saao/saa10/P334751:SAA 10 044. Timing a Journey of the King (ABL 1141+) [from astrologers]\n",
      "Saving Output/saao_saa10_P334751.txt\n",
      "\n",
      "Parsing dcclt/Q000043\n",
      "dcclt/Q000043:OB Nippur Ura 06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 5/6 [00:06<00:01,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Output/dcclt_Q000043.txt\n",
      "\n",
      "Parsing blms/P274259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 6/6 [00:06<00:00,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blms/P274259:CT 58 63 (BM 054636+) [Exam at the Scribal School]\n",
      "Saving Output/blms_P274259.txt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open('Input/' + inputfile, mode = 'r') as f:\n",
    "    textlist = f.read().splitlines()\n",
    "for eachtextid in tqdm(textlist):\n",
    "    sleep(0.01)\n",
    "    eachtextid = eachtextid.rstrip()\n",
    "    file = 'HTML/' + eachtextid.replace('/', '_') + '.html'\n",
    "    try:\n",
    "        csvformat = scrape(eachtextid)\n",
    "        outputfile = 'Output/' + eachtextid.replace('/', '_') + '.txt'\n",
    "        print(\"Saving \" + outputfile + '\\n')\n",
    "    \n",
    "        if not os.path.exists('Output'):\n",
    "            os.mkdir('Output')\n",
    "        \n",
    "        with open(outputfile, mode = 'w') as writeFile:\n",
    "            writeFile.write(csvformat)  \n",
    "\n",
    "    except IOError:\n",
    "        print(file + ' not available')\n",
    "        textiderror = textiderror + eachtextid + '\\n'\n",
    "\n",
    "#Create error log\n",
    "if not os.path.exists('Error'):\n",
    "    os.mkdir('Error')\n",
    "with open('Error/oraccerror.txt', mode='w') as writeFile:\n",
    "    writeFile.write(textiderror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
