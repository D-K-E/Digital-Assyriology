{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Model of the State Archives of Assyria Letters\n",
    "\n",
    "This is a work-in-progress Notebook. The number of topics is set with the ntopics variable. There are almost 2500 letters, most of them brief or very brief. The topic model produced from the entire corpus (with 15 or 25 topics) is hard to interpret. One reason may be that the texts are too short. A brief or broken letter with just a few words that happen to be important in a certain topic may come out as one of the most important documents in that topic (because a high percentage of that letter resides in the topic). One solution may be to group (aggregate) letters by metadata, in the file SAAO.csv. One could group the letters by Chapter number (this referes to chapters in the original book publications - assuming that those chapters group texts that are related in a meaningful way)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Data\n",
    "First read the directory with the State Archives of Assyria letters. These files contain lemmatization in ORACC. The texts list lemmatizations per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_text</th>\n",
       "      <th>l_no</th>\n",
       "      <th>text</th>\n",
       "      <th>text_name</th>\n",
       "      <th>version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>saao/saa01/P224485</td>\n",
       "      <td>o 1</td>\n",
       "      <td>awātu[word]N šarru[king]N ana[to]PRP Aššur-šar...</td>\n",
       "      <td>SAA 01 001. Midas Of Phrygia Seeks Detente (NL...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>saao/saa01/P224485</td>\n",
       "      <td>o 2</td>\n",
       "      <td>šulmu[completeness]N ana[to]PRP libbu[interior...</td>\n",
       "      <td>SAA 01 001. Midas Of Phrygia Seeks Detente (NL...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>saao/saa01/P224485</td>\n",
       "      <td>o 3</td>\n",
       "      <td>ša[that]REL šapāru[send]V mā[saying]PRP māru[s...</td>\n",
       "      <td>SAA 01 001. Midas Of Phrygia Seeks Detente (NL...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>saao/saa01/P224485</td>\n",
       "      <td>o 4</td>\n",
       "      <td>Muskaya[Phrygian]EN ina[in]PRP muhhu[skull]N a...</td>\n",
       "      <td>SAA 01 001. Midas Of Phrygia Seeks Detente (NL...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>saao/saa01/P224485</td>\n",
       "      <td>o 5</td>\n",
       "      <td>Quwaya[from-Quwe]EN ša[that]REL Urik[1]PN ana[...</td>\n",
       "      <td>SAA 01 001. Midas Of Phrygia Seeks Detente (NL...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id_text l_no                                               text  \\\n",
       "0  saao/saa01/P224485  o 1  awātu[word]N šarru[king]N ana[to]PRP Aššur-šar...   \n",
       "1  saao/saa01/P224485  o 2  šulmu[completeness]N ana[to]PRP libbu[interior...   \n",
       "2  saao/saa01/P224485  o 3  ša[that]REL šapāru[send]V mā[saying]PRP māru[s...   \n",
       "3  saao/saa01/P224485  o 4  Muskaya[Phrygian]EN ina[in]PRP muhhu[skull]N a...   \n",
       "4  saao/saa01/P224485  o 5  Quwaya[from-Quwe]EN ša[that]REL Urik[1]PN ana[...   \n",
       "\n",
       "                                           text_name version  \n",
       "0  SAA 01 001. Midas Of Phrygia Seeks Detente (NL...     NaN  \n",
       "1  SAA 01 001. Midas Of Phrygia Seeks Detente (NL...     NaN  \n",
       "2  SAA 01 001. Midas Of Phrygia Seeks Detente (NL...     NaN  \n",
       "3  SAA 01 001. Midas Of Phrygia Seeks Detente (NL...     NaN  \n",
       "4  SAA 01 001. Midas Of Phrygia Seeks Detente (NL...     NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path =r'../Scrape-Oracc/Output/' # use your path\n",
    "allFiles = glob.glob(path + \"/saao\" + \"*.txt\")\n",
    "list_ = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0)\n",
    "    list_.append(df)\n",
    "saao_data = pd.concat(list_)\n",
    "saao_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collapse lines to one row per document\n",
    "In order to transform this DataFrame into a proper input for topic modeling we need to discard the column `l_no` and concatenate all the text that belongs to one letter in a single row. Some lines have no content in the `text` column - these lines need to be dropped.\n",
    "\n",
    "First select the relevant columns and drop the rows that have no text content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_text</th>\n",
       "      <th>text_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>saao/saa01/P224485</td>\n",
       "      <td>SAA 01 001. Midas Of Phrygia Seeks Detente (NL...</td>\n",
       "      <td>awātu[word]N šarru[king]N ana[to]PRP Aššur-šar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>saao/saa01/P224485</td>\n",
       "      <td>SAA 01 001. Midas Of Phrygia Seeks Detente (NL...</td>\n",
       "      <td>šulmu[completeness]N ana[to]PRP libbu[interior...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>saao/saa01/P224485</td>\n",
       "      <td>SAA 01 001. Midas Of Phrygia Seeks Detente (NL...</td>\n",
       "      <td>ša[that]REL šapāru[send]V mā[saying]PRP māru[s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>saao/saa01/P224485</td>\n",
       "      <td>SAA 01 001. Midas Of Phrygia Seeks Detente (NL...</td>\n",
       "      <td>Muskaya[Phrygian]EN ina[in]PRP muhhu[skull]N a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>saao/saa01/P224485</td>\n",
       "      <td>SAA 01 001. Midas Of Phrygia Seeks Detente (NL...</td>\n",
       "      <td>Quwaya[from-Quwe]EN ša[that]REL Urik[1]PN ana[...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id_text                                          text_name  \\\n",
       "0  saao/saa01/P224485  SAA 01 001. Midas Of Phrygia Seeks Detente (NL...   \n",
       "1  saao/saa01/P224485  SAA 01 001. Midas Of Phrygia Seeks Detente (NL...   \n",
       "2  saao/saa01/P224485  SAA 01 001. Midas Of Phrygia Seeks Detente (NL...   \n",
       "3  saao/saa01/P224485  SAA 01 001. Midas Of Phrygia Seeks Detente (NL...   \n",
       "4  saao/saa01/P224485  SAA 01 001. Midas Of Phrygia Seeks Detente (NL...   \n",
       "\n",
       "                                                text  \n",
       "0  awātu[word]N šarru[king]N ana[to]PRP Aššur-šar...  \n",
       "1  šulmu[completeness]N ana[to]PRP libbu[interior...  \n",
       "2  ša[that]REL šapāru[send]V mā[saying]PRP māru[s...  \n",
       "3  Muskaya[Phrygian]EN ina[in]PRP muhhu[skull]N a...  \n",
       "4  Quwaya[from-Quwe]EN ša[that]REL Urik[1]PN ana[...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saao_data = saao_data[['id_text', 'text_name', 'text']]\n",
    "saao_data = saao_data.dropna()\n",
    "saao_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the rows by `id_text` and apply the `join` function to the `text` column. Transform the aggregated data into a new DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_text</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>saao/saa01/P224485</th>\n",
       "      <td>awātu[word]N šarru[king]N ana[to]PRP Aššur-šar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saao/saa01/P313416</th>\n",
       "      <td>ana[to]PRP šarru[king]N bēlu[lord]N ardu[slave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saao/saa01/P313417</th>\n",
       "      <td>ana[to]PRP šarru[king]N bēlu[lord]N ardu[slave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saao/saa01/P313425</th>\n",
       "      <td>ana[to]PRP šarru[king]N bēlu[lord]N ardu[slave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saao/saa01/P313427</th>\n",
       "      <td>ana[to]PRP šarru[king]N bēlu[lord]N ardu[slave...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 text\n",
       "id_text                                                              \n",
       "saao/saa01/P224485  awātu[word]N šarru[king]N ana[to]PRP Aššur-šar...\n",
       "saao/saa01/P313416  ana[to]PRP šarru[king]N bēlu[lord]N ardu[slave...\n",
       "saao/saa01/P313417  ana[to]PRP šarru[king]N bēlu[lord]N ardu[slave...\n",
       "saao/saa01/P313425  ana[to]PRP šarru[king]N bēlu[lord]N ardu[slave...\n",
       "saao/saa01/P313427  ana[to]PRP šarru[king]N bēlu[lord]N ardu[slave..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saao_byletter = saao_data['text'].groupby(saao_data['id_text']).apply(' '.join)\n",
    "saao_byletter_df = pd.DataFrame(saao_byletter)\n",
    "saao_byletter_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Create a DataFrame of `id_text` and `text_name` equivalencies, with `id_text` set as index (row names). Then merge this DataFrame with the the `saao_bytext_df` using the indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_text</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>saao/saa01/P224485</th>\n",
       "      <td>SAA 01 001. Midas Of Phrygia Seeks Detente (NL...</td>\n",
       "      <td>awātu[word]N šarru[king]N ana[to]PRP Aššur-šar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saao/saa01/P313416</th>\n",
       "      <td>SAA 01 158. Gold and Silver Objects Sent to th...</td>\n",
       "      <td>ana[to]PRP šarru[king]N bēlu[lord]N ardu[slave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saao/saa01/P313417</th>\n",
       "      <td>SAA 01 233. More Land to Bel-duri (CT 53 002)</td>\n",
       "      <td>ana[to]PRP šarru[king]N bēlu[lord]N ardu[slave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saao/saa01/P313425</th>\n",
       "      <td>SAA 01 179. No Iron to the Arabs! (CT 53 010)</td>\n",
       "      <td>ana[to]PRP šarru[king]N bēlu[lord]N ardu[slave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saao/saa01/P313427</th>\n",
       "      <td>SAA 01 152. The Affair of Gidgidanu and His Br...</td>\n",
       "      <td>ana[to]PRP šarru[king]N bēlu[lord]N ardu[slave...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            text_name  \\\n",
       "id_text                                                                 \n",
       "saao/saa01/P224485  SAA 01 001. Midas Of Phrygia Seeks Detente (NL...   \n",
       "saao/saa01/P313416  SAA 01 158. Gold and Silver Objects Sent to th...   \n",
       "saao/saa01/P313417      SAA 01 233. More Land to Bel-duri (CT 53 002)   \n",
       "saao/saa01/P313425      SAA 01 179. No Iron to the Arabs! (CT 53 010)   \n",
       "saao/saa01/P313427  SAA 01 152. The Affair of Gidgidanu and His Br...   \n",
       "\n",
       "                                                                 text  \n",
       "id_text                                                                \n",
       "saao/saa01/P224485  awātu[word]N šarru[king]N ana[to]PRP Aššur-šar...  \n",
       "saao/saa01/P313416  ana[to]PRP šarru[king]N bēlu[lord]N ardu[slave...  \n",
       "saao/saa01/P313417  ana[to]PRP šarru[king]N bēlu[lord]N ardu[slave...  \n",
       "saao/saa01/P313425  ana[to]PRP šarru[king]N bēlu[lord]N ardu[slave...  \n",
       "saao/saa01/P313427  ana[to]PRP šarru[king]N bēlu[lord]N ardu[slave...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saao_id_names = saao_data[['id_text', 'text_name']].drop_duplicates().set_index('id_text')\n",
    "saao_data_df = pd.merge(saao_id_names, saao_byletter_df, right_index=True, left_index=True)\n",
    "saao_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, utils\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test corpus: first 100 texts\n",
    "[For test purposes only the first 100 documents are admitted.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#saao_data_df = saao_data_df[:100]\n",
    "documents = saao_data_df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize and POS-filter\n",
    "Tokenization is done by splitting on white space. The variable `posfilter` holds the last two characters of lemmatized words with allowed Part of Speech tags. If, for instance, you wish to select Verbs, Adjectives, and Nouns (in Akkadian), posfilter will be `[']n', 'aj', ']v']`. Note that one-character pos-tags need the right bracket!\n",
    "The POS labels are:\n",
    "* \"n\", #Nouns\n",
    "* \"v\", #Verbs\n",
    "* \"aj\", #Adjectives\n",
    "* \"av\", #Adverbs\n",
    "* \"an\", #Agricultural Name\n",
    "* \"cn\", #Celestial Name\n",
    "* \"dn\", #Divine Name\n",
    "* \"en\", #Ethnicity Name\n",
    "* \"fn\", #Field Name\n",
    "* \"gn\", #Geographical Name (lands, etc.)\n",
    "* \"ln\", #Lineage Name (ancestral clan)\n",
    "* \"mn\", #Month Name\n",
    "* \"on\", #Object Name\n",
    "* \"pn\", #Personal Name\n",
    "* \"qn\", #Quarter (of a city) Name\n",
    "* \"rn\", #Royal Name\n",
    "* \"sn\", #Settlement Name\n",
    "* \"tn\", #Temple Name\n",
    "* \"wn\", #Watercourse Name\n",
    "* \"yn\", #Year Name\n",
    "* \"nu\", #Numeral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posfilter = [']n', ']v', 'en']\n",
    "texts = [[word for word in document.lower().split() if word[-2:] in posfilter] for document in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words\n",
    "\n",
    "Stop words are very frequent words that are not able to distinguish between topics. This includes, for instance, prepositions - but those have already been filtered out by the POS filter. The following nouns and verbs are too frequent to contribute to the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stoplist = [\n",
    "'šarru[king]n',\n",
    "'bēlu[lord]n',\n",
    "'libbu[interior]n',\n",
    "'muhhu[skull]n',\n",
    "'ardu[slave]n',\n",
    "'šulmu[completeness]n',\n",
    "'šapāru[send]v',\n",
    "'alāku[go]v',\n",
    "'qabû[say]v',\n",
    "'pānu[front]n',\n",
    "'māru[son]n',\n",
    "'bītu[house]n',\n",
    "'epēšu[do]v',\n",
    "'wabālu[bring]v',\n",
    "'šakānu[put]v',\n",
    "'amāru[see]v',\n",
    "'bašû[exist]v',\n",
    "'našû[lift]v',\n",
    "'izuzzu[stand]v',\n",
    "'ūmu[day]n',\n",
    "'ṭābu[good]aj',\n",
    "'mādu[many]aj',\n",
    "'nadānu[give]v',\n",
    "'tadānu[give]v',\n",
    "'ṣehru[small]aj',\n",
    "'mimmû[all]n',\n",
    "'gimru[totality]n',\n",
    "'gabbu[totality]n',\n",
    "'šâlu[ask]v',\n",
    "'šemû[hear]v',\n",
    "'ūmu[day]n',\n",
    "'awātu[word]n',\n",
    "'erēbu[enter]v']\n",
    "texts = [[word for word in text if word not in stoplist] for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary\n",
    "create the gensim Dictionary and filter for words that are too common or too rare (no_above may be set too low here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ntopics = 25\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(corpus, num_topics=ntopics, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the top words and their probabilities in all topics. Note: the topic numbers here are not the ones used below in the visualizations! (The topics are the same, but not their numbers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ldamodel.show_topics(ntopics, formatted = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyLDAvis\n",
    "Use pyLDAvis to visualize the topic model. By default, pyLDAvis will order the topics by [prevalence](https://github.com/bmabey/pyLDAvis/issues/59) (topic 1 is the most prevalent topic). That means that the topic numbers in the visualization do not agree with the topic numbers in the lda model. To prevent this behaviour one may use `sort_topics=False` in the `prepare` command. The advantage of ordering the topics by prevalence, however, is that new instances of the lda model are more comparable (that is, the same topic will receive the same number). Note that the library was written in Java for R, and so the numbering in the visualization begins with 1 (not with 0). The topic numbers in the Document/Topic and Topic/Term matrices below will be adjusted to be compatible with the pyLDAvis visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "vis = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.save_html(vis, 'saao.html')\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Document/Topic Table\n",
    "Create a DataFrame that gives the probability of each topic for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_of_doctopics = [ldamodel.get_document_topics(corpus[i], minimum_probability=0) for i in range(len(corpus))]\n",
    "list_of_probabilities = [[probability for label,probability in distribution] for distribution in list_of_doctopics]\n",
    "df_list = [pd.DataFrame(list_of_probabilities[i]).transpose() for i in range(len(corpus))]\n",
    "doc_topic_df = pd.concat(df_list)\n",
    "doc_topic_df = doc_topic_df.set_index(saao_data_df.index)\n",
    "doc_topic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reorder the topics (columns) according to prevalence and rename the topics `topic 1` to `topic n` in accordance with the pyLDAvis visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topics = ['topic ' + str(i+1) for i in range(ntopics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "doc_topic_df.columns = topics\n",
    "doc_topic_df['text_name'] = saao_data_df['text_name']\n",
    "doc_topic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Create Topic / Term table\n",
    "This is a table with N rows (the number of topics) and M columns (the number of individual terms in the Dictionary). The table indicates the probability of each term in each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_term = ldamodel.show_topics(ntopics, formatted=False, num_words=len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object `topic_term` is a list of list. Each topic is represented by a list of tuples in the form `(word, probability)`. The following code pulls out the probabilities for each word in each topic (`topic_term[i][1]`) and creates a list of DataFrames with the words as index (rows) and the probabilities as the only column. Each Dataframe is transposed and the DataFrames are concatenated to a single DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_term_list = [pd.DataFrame(topic_term[i][1]).set_index(0).transpose() for i in range(0, ntopics)]\n",
    "topic_term_df = pd.concat(topic_term_list, ignore_index=True)\n",
    "topic_term_df['topics'] = topics\n",
    "topic_term_df = topic_term_df.set_index('topics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topics (rows) are numbered 0-14 in the (random) order of the lda model. The array `prevalences` is used to re-arrange the topics in the order of prevalence (as is done above for the document/topic matrix). The re-arranged topics (rows) are now re-labeled `topic 1` to `topic 15` in accordance with their numbering in the pyLDAvis visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_term_df['ēkallu[palace]n']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization below plots all the documents according to their distances (using Multi-Dimensional Scaling) in the Document/Topic DataFrame. Each document (dot) is colored according to the most prevalent topic and the size of the dot represents the probability of the most prevalent topic in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.manifold import MDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the distances between each of the documents. Use either the Document/Topic Dataframe or the Document/Term Dataframe (constructed below) to measure distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(analyzer='word', token_pattern=r'[^ ]+', vocabulary=list(topic_term_df.columns.values))\n",
    "saao_dtm = cv.fit_transform(saao_data_df['text'])\n",
    "saao_dtm_df = pd.DataFrame(saao_dtm.toarray(), columns = cv.get_feature_names(), index = saao_data_df.index.values)\n",
    "saao_dtm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dist = squareform(pdist(saao_dtm_df))\n",
    "dist = squareform(pdist(doc_topic_df.drop('text_name', axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the position of each document using Multi-Dimensional Scaling. The variable `pos` holds the `x` and `y`  coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mds = MDS(n_components=2, max_iter=3000,\n",
    "      random_state=seed, dissimilarity=\"precomputed\", n_jobs=1)\n",
    "\n",
    "pos = mds.fit(dist).embedding_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of x values (coordinates) and a list of y values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_x = [x for x, y in pos]\n",
    "pos_y = [y for x, y in pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create lists of the most prevalent topic, the probability of the most prevalent topic, and the text name for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prevalent_topic = doc_topic_df.drop('text_name', axis=1).idxmax(axis=1)\n",
    "probability = [doc_topic_df.ix[i][prevalent_topic[i]] for i in range(0, len(corpus))]\n",
    "text_name = [name for name in doc_topic_df['text_name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import bokeh and draw the visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from bokeh.models import ColumnDataSource, OpenURL, TapTool, HoverTool\n",
    "from bokeh.plotting import figure, output_file, output_notebook, show\n",
    "from collections import OrderedDict\n",
    "output_notebook()\n",
    "\n",
    "p = figure(\n",
    "    plot_width=800, plot_height=800,\n",
    "    tools=\"tap,pan,wheel_zoom,box_zoom,reset,save\", \n",
    "    title=\"Topic Distribution MDS\\nSize of the circle represents prevalence of the topic\")\n",
    "p.add_tools(HoverTool(\n",
    "        tooltips=[\n",
    "            (\"url\", \"http://oracc.org/\" + \"@id_text\"),\n",
    "            ((\"topic, probability\"), (\"@topic, @probability\")),\n",
    "            (\"text name\", \"@text_name\")\n",
    "        ]\n",
    "        ))\n",
    "\n",
    "colormap = [('topic 1', \"orange\"), ('topic 2', \"olive\"), ('topic 3', \"firebrick\"), \n",
    "          ('topic 4', \"gold\"), ('topic 5', \"red\"), ('topic 6', \"black\"), ('topic 7', \"green\"), \n",
    "          ('topic 8', \"blue\"), ('topic 9', \"purple\"), ('topic 10', \"darkslategray\"), ('topic 11', \"yellow\"), \n",
    "          ('topic 12', \"indigo\"), ('topic 13', \"blueviolet\"), ('topic 14', \"saddlebrown\"), ('topic 15',\"navy\"), ('topic 16', 'orange'),\n",
    "           ('topic 17', 'olive'), ('topic 18', 'firebrick'), ('topic 19', 'gold'), ('topic 20', 'red'), ('topic 21', 'black'),\n",
    "            ('topic 22', 'green'), ('topic 23', 'blue'), ('topic 24', 'purple'), ('topic 25', 'darkslategray')]\n",
    "colormap = OrderedDict(colormap)\n",
    "colors = [colormap[n] for n in prevalent_topic]\n",
    "color_list = [colormap[topic] for topic in colormap]\n",
    "color_list += [\"black\"] * (len(colors) - len(color_list))\n",
    "source = ColumnDataSource(data=dict(\n",
    "        x=pos_x,\n",
    "        y=pos_y,\n",
    "        id_text=list(doc_topic_df.index.values),\n",
    "        size = probability/max(probability)*25,\n",
    "        probability = probability,\n",
    "        topic = prevalent_topic,\n",
    "        color = colors,\n",
    "        orig_color = color_list,\n",
    "        text_name = text_name\n",
    "    ))\n",
    "\n",
    "p.circle('x', 'y', color='color', fill_alpha=0.5, size='size', source=source)\n",
    "\n",
    "url = \"http://oracc.museum.upenn.edu/@id_text\"\n",
    "    \n",
    "\n",
    "taptool = p.select(type=TapTool)\n",
    "taptool.callback = OpenURL(url=url)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following graph uses a slider to choose one topic. The idea is to color the dots (documents) that have high prevalence for that topic and leave the other dots gray (or: use a high fill_alpha for the chosen topic and a low one for the ones that are backgrounded). Since the dots link to the online editions of these documents, that would make it easier to explore a topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bokeh.io import vform\n",
    "from bokeh.models import ColumnDataSource, OpenURL, TapTool, HoverTool, CustomJS\n",
    "from bokeh.models.widgets import Slider\n",
    "from bokeh.plotting import figure, output_file, output_notebook, show\n",
    "from bokeh.layouts import widgetbox, column\n",
    "from collections import OrderedDict\n",
    "output_notebook()\n",
    "\n",
    "p = figure(\n",
    "    plot_width=800, plot_height=800,\n",
    "    tools=\"tap,pan,wheel_zoom,box_zoom,reset,save\", \n",
    "    title=\"Topic Distribution MDS\\nSize of the circle represents prevalence of the topic\")\n",
    "p.add_tools(HoverTool(\n",
    "        tooltips=[\n",
    "            (\"url\", \"http://oracc.org/\" + \"@id_text\"),\n",
    "            ((\"topic, probability\"), (\"@topic, @probability\")),\n",
    "            (\"text name\", \"@text_name\")\n",
    "        ]\n",
    "        ))\n",
    "\n",
    "\n",
    "colormap = [('topic 1', \"orange\"), ('topic 2', \"olive\"), ('topic 3', \"firebrick\"), \n",
    "          ('topic 4', \"gold\"), ('topic 5', \"red\"), ('topic 6', \"black\"), ('topic 7', \"green\"), \n",
    "          ('topic 8', \"blue\"), ('topic 9', \"purple\"), ('topic 10', \"darkslategray\"), ('topic 11', \"yellow\"), \n",
    "          ('topic 12', \"indigo\"), ('topic 13', \"blueviolet\"), ('topic 14', \"saddlebrown\"), ('topic 15',\"navy\"), ('topic 16', 'orange'),\n",
    "           ('topic 17', 'olive'), ('topic 18', 'firebrick'), ('topic 19', 'gold'), ('topic 20', 'red'), ('topic 21', 'black'),\n",
    "            ('topic 22', 'green'), ('topic 23', 'blue'), ('topic 24', 'purple'), ('topic 25', 'darkslategray')]\n",
    "colormap = OrderedDict(colormap)\n",
    "colors = [colormap[n] for n in prevalent_topic]\n",
    "color_list = [colormap[topic] for topic in colormap]\n",
    "color_list += [\"black\"] * (len(colors) - len(color_list))\n",
    "\n",
    "source = ColumnDataSource(data=dict(\n",
    "        x=pos_x,\n",
    "        y=pos_y,\n",
    "        id_text=list(doc_topic_df.index.values),\n",
    "        size = probability/max(probability)*25,\n",
    "        probability = probability,\n",
    "        topic = prevalent_topic,\n",
    "        color = colors,\n",
    "        orig_color = color_list,\n",
    "        text_name = text_name\n",
    "    ))\n",
    "\n",
    "p.circle('x', 'y', color='color', fill_alpha=0.5, size='size', source=source)\n",
    "\n",
    "callback = CustomJS(args=dict(source=source), code = \"\"\"\n",
    "    var data = source.get('data');\n",
    "    var current_topic = topic.value;\n",
    "    x = data['x']\n",
    "    y = data['y']\n",
    "    col = data['color']\n",
    "    topic = data['topic']\n",
    "    orig_color = data['orig_color']\n",
    "    for (i = 0; i < x.length; i++) {\n",
    "        if (topic[i].substring(6) == current_topic) {\n",
    "            col[i] = orig_color[current_topic-1]\n",
    "        } else {\n",
    "            col[i] = 'grey'\n",
    "        }\n",
    "    }\n",
    "    source.trigger('change');\n",
    "\"\"\")\n",
    "\n",
    "topic_slider = Slider(start=1, end = ntopics, value=1, step=1, title = 'Topic', callback=callback)\n",
    "callback.args['topic'] = topic_slider\n",
    "\n",
    "url = \"http://oracc.museum.upenn.edu/@id_text\"\n",
    "\n",
    "taptool = p.select(type=TapTool)\n",
    "taptool.callback = OpenURL(url=url)\n",
    "\n",
    "show(column(topic_slider, p))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
